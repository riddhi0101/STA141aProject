---
title: "Assessing Treatment Effects of TFD725 for Non-small Cell Lung Cancer"
output:
  html_document:
    df_print: paged
    fig_caption: yes
    number_sections: yes
  pdf_document: default
---

<style type="text/css">

body{ /* Normal  */
      font-size: 18px;
  }

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,message=FALSE,warning=FALSE)
```

***

Team ID:

Name (tasks):

Name (tasks):

Name (tasks):

Auditor (tasks):

***
```{r}
library("scatterplot3d")
library(dplyr)
library(tidyr)
library(qwraps2)
library(kernlab)
library(cluster)
library(filling)
## project prelims
setwd("/Users/riddhib/Desktop/fall2020/STA_141a/final")
travel_review <- read.csv("./google_review_ratings.csv", stringsAsFactors=FALSE)
## correcting for errors
error_lines= which(!is.na(travel_review$X))
# One line has Category.24 misplaced at another column
travel_review$Category.24[error_lines[1]] = travel_review$X[error_lines[1]]
# One line has put a character value at Category.11. Correct it to 2.  
travel_review$Category.11[error_lines[2]] = 2
# The line misplaced all columns after Category.12.
travel_review[error_lines[2],13:25] = travel_review[error_lines[2],14:26]
# Change Category.11 to numeric 
travel_review$Category.11 = as.numeric(travel_review$Category.11)
# Delete an extra column  
review_data = travel_review[ names(travel_review) != "X"]
# Delete the temporary variables/data
if(exists('travel_review')) 
    rm(travel_review)
if(exists('error_lines'))
    rm(error_lines)

review_data = review_data %>% rename(
    church = Category.1,
    resort = Category.2,
    beach = Category.3,
    park = Category.4,
    theatre = Category.5,
    museum = Category.6,
    mall = Category.7,
    zoo = Category.8,
    restaurant = Category.9,
    bar = Category.10,
    local_service = Category.11,
    burger_pizza = Category.12,
    hotel = Category.13,
    juice_bars = Category.14,
    art_galleries = Category.15,
    dance_clubs = Category.16,
    swimming_pool = Category.17,
    gym = Category.18,
    bakeries = Category.19,
    spa = Category.20,
    cafe = Category.21,
    view_points = Category.22,
    monuments = Category.23,
    gardens = Category.24
)

```
# Introduction

## Background

In this portion of the project, we attempt to implement matrix completion in the google travel reviews dataset. The data includes average ratings from approximately 5000 people on 24 different catagories of attractions(spa,resort, etc)

In reality, one may obtain only partial review data and would like to infer the rest to provide accurate recomendations. To similate this situation, we randomly select a portion of the data to treat as missing and then use matrix completion to predict the missing ratings. Because we have complete data, we can then compare the predictions to the true ratings. This line of analysis is inspired primarily from "Exact Matrix Completion via Convex Optimization" by Candes and Retchet. To do the actual computation, we use the soft impute algorithm(Mazumder, 2010).

Citations:

Cand√®s, E.J., Recht, B. Exact Matrix Completion via Convex Optimization. Found Comput Math 9, 717 (2009). https://doi.org/10.1007/s10208-009-9045-5
Mazumder, Rahul & Hastie, Trevor & Tibshirani, Robert. (2010). Spectral Regularization Algorithms for Learning Large Incomplete Matrices. Journal of machine learning research : JMLR. 11. 2287-2322.

## Statistical questions of interest


We want to complete the matrix. In doing so, we assume that the data has some low rank structure, there is some information about ratings in each row and column(ensured by random sampling), and that the matrix is incoherent. These conditions are requirements for the completion problem to be well posed(Candes 2008). Then to evaluate our success, we compare our completes matrix to the whole data.


# Analysis Plan


### Matrix Completion

Let $X$ be the data matrix which is 24 by 5,426 (5427 users and 24 categories reviewd). $X_{ij}$ is the average review rating of the jth person in the ith catagory. Let $\Omega$ be the set of entries that are observed.This set is created by randomly sampling points to treat as observed from the full matrix $X$.
Let 
$$ P_{\Omega}(X) = \left\{
        \begin{array}{ll}
            X_{ij} & \quad (i,j) \in \Omega \\
            0 & \quad otherwise
        \end{array}
    \right.
  $$
Let $Z$ be the matrix that is completed using soft impute method.

The objective of the matrix completion is to find a find the matrix $Z$ such that it has the smallest possible rank and the observed entries are as close to the original as possible. This can be written as follows, where $\delta$ is some small real number. 
$$ \left\| P_\Omega (X) - P_\Omega(Z) \right\|_F < \delta\\
min \: \left\|Z \right\|_*$$
Here $\left\|Z \right\|_*$ is the nuclear norm defined as follows.  $\sigma(Z)$ are the singular values of Z. The nuclear norm is used as an estimator of the rank because it is easier to minimize.
$$
\left\|Z \right\|_* = \sum_{k = 1}^{n} \sigma_k(Z)
$$

To solve this, we decided to use the soft impute algorithm (Mazumderet al.) to minimize the following funtion. It essentially uses soft thresholding to solve the objective function specified above.


$$
\min_Z f_\lambda(Z) = \frac{1}{2} \left\| P_\Omega (X) - P_\Omega(Z) \right\|_F^2 + \lambda\left\| Z\right\|_*
$$
To choose the hyperparameter, we run soft impute on the following list (1,6,11...,101). Then we calculate the for frobenius norm of the residual as shown below. We chose the lambda values which results in the smallest value for the residual because it gives the matrix that is closest to the original. 
$$
total\:residual = \left\|Z-X\right\|_F
$$
In reality, in a matrix completion problem, you wouldnt know the ground truth. Then, to pick a lambda, one should treat a subset of values of the incomplete matrix as missing (creating a train/test set). This would allow you to validate the lambda that you pick to see ensure the matrix completion generalizes well.

To look more closely at our completed matrix, we calculate partitions of the residuals in the following ways

$$
observed \: residual = \left\|P_\Omega(Z) - P_\Omega(X) \right\|_F \\
unobserved \: residual = \left\|P_{\notin \Omega} (Z) - P_{\notin \Omega}(X) \right\|_F 
$$


Citations:
Mazumder, Rahul & Hastie, Trevor & Tibshirani, Robert. (2010). Spectral Regularization Algorithms for Learning Large Incomplete Matrices. Journal of machine learning research : JMLR. 11. 2287-2322. 
# Results


## Inferential Analysis
```{r}
library(filling)

data.mat = data.matrix(review_data)
data.mat = data.mat[,2:25]
data.mat = t(data.mat)
#dim(data.mat)

incomp10 = aux.rndmissing(data.mat, x = .1)
incomp50 = aux.rndmissing(data.mat, x = 0.5)
incomp75 = aux.rndmissing(data.mat, x = 0.75)


```


### 10% incomplete
```{r}
# running soft impute for the following lambda values
aa = seq(1,101, by=10)
lambdasL = rev(aa)
comp10 = fill.SoftImpute(incomp10,lambdas =lambdasL)
comp10 = comp10$X

## calculating total residual for each lambda value
d = dim(comp10)
normC = c()
for (i in 1:d[3]){
    r = data.mat - comp10[,,i]
    a = norm(r,type = "F")
    normC = append(normC, a)}
minE = which(normC == min(normC))
bestcomp10 = comp10[,,minE]
plot(aa, rev(normC), xlab= "lambda values", ylab = "total residual", main= "Residuals of Lambda values (10% incomplete)")




```


```{r}
obsvRdata = incomp10 - bestcomp10
obsvRdata = as.vector(obsvRdata)
obsvR = sum(obsvRdata*obsvRdata, na.rm = T)
totR = min(normC)
unobR = totR-obsvR
```

Total Residual = `r totR `
Observed Residual = `r obsvR `
Unobserved Residual = `r unobR `



### 50% incomplete
```{r}
aa = seq(1,101, by=10)
lambdasL = rev(aa)
comp50 = fill.SoftImpute(incomp50,lambdas =lambdasL)
comp50 = comp50$X

## calculating total residual for each lambda value
d = dim(comp50)
normC = c()
for (i in 1:d[3]){
    r = data.mat - comp50[,,i]
    a = norm(r,type = "F")
    normC = append(normC, a)}
minE = which(normC == min(normC))
bestcomp50 = comp50[,,minE]
plot(aa, rev(normC), xlab= "lambda values", ylab = "total residual", main= "Residuals of Lambda values (50% incomplete)")

```


```{r}
obsvRdata = incomp50 - bestcomp50
obsvRdata = as.vector(obsvRdata)
obsvR = sum(obsvRdata*obsvRdata, na.rm = T)
totR = min(normC)
unobR = totR-obsvR

```

Total Residual = `r totR `
Observed Residual = `r obsvR `
Unobserved Residual = `r unobR `

### Conclusion
As expected, there is a larger residual error in completing the matrix that has more missing data. The matrix completeion works quite well there is about `r unobR/(5456*25*.1)` error per completed entry. Because the observed residual is quite small, both 10% and 50% completion are able to recover the observed values with high accuracy.



# Session information
```{r, eval=FALSE}
print(sessionInfo(), local = FALSE)
```
